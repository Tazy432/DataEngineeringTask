{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by removing all trailing/leading whitespaces, and by ensuring that all data is decoded corectly using the mosdle ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ftfy\n",
    "import re\n",
    "# We consider all data as strings . We use python as engine due to the messines of the data . We use quotechar because i've saw in the dataset we have some texts that use \"\", in order to 'escape' the comma limitation\n",
    "df = pd.read_csv(\"facebook_dataset.csv\", engine=\"python\", quotechar='\"', on_bad_lines='skip', dtype=str)\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Remove leading and trailing whitespaces using regex\n",
    "df = df.map(lambda x: re.sub(r'\\s+', ' ', x).strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Ensure all elements from DataFrame are decoded correctly\n",
    "df = df.map(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)\n",
    "df.to_csv(\"facebook.csv\",encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by completing the country name , and country code . If we know the code , then we know the country . If we know the country , then we know the code. But in order for us to do that , we must first scrape data about countries from a website . We are interested in the country name, Iso-2 country code, and country code phone number:\n",
    "\n",
    "country_code: ->it could be empty. But if we did have the name of the country, we could easily find its country code using the scraped data\n",
    "            ex:(suppose the country_name is france)\n",
    "                Before:\"\"\n",
    "                After :fr \n",
    "\n",
    "country_name: ->it could be empty. But if we did have the code of the country, we could easily find its country name using the scraped data\n",
    "            ex:(suppose the country_code is fr)\n",
    "                Before:\"\"\n",
    "                After :france\n",
    "For PuertoRico and --------------------- , we need to delete some of the country phone code options in order to keep a clean csv file (these countries have more than 1 country phone code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'countryCodes.csv' has been cleared.\n"
     ]
    }
   ],
   "source": [
    "#in order to assure that the phone numbers are listed corectly (they start with the right digits) \n",
    "# we scrap some data from a site \n",
    "#Here we get :\n",
    "# ->The Name of the country\n",
    "# ->The digit combination for phone number for each country (parsed)\n",
    "# ->The 2-ISO code of each country (parsed)\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"countryCodes.csv\"):\n",
    "    with open(\"countryCodes.csv\", 'w') as file:\n",
    "        pass  # This will clear the file content\n",
    "    print(f\"The file '{\"countryCodes.csv\"}' has been cleared.\")\n",
    "\n",
    "\n",
    "def parseIsoCodes(code):\n",
    "    return code.split(' / ')[0][:2]\n",
    "\n",
    "def parseCountryCode(country_code):\n",
    "    return country_code.replace('-', '')\n",
    "\n",
    "# Fetch the data from the website\n",
    "response = requests.get(\"https://countrycode.org/\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all rows in the table body\n",
    "rows = soup.find('tbody').find_all('tr')\n",
    "fileName = \"countryCodes.csv\"\n",
    "\n",
    "# Write header for CSV file and clear previous content\n",
    "with open(fileName, 'w') as file:\n",
    "    file.write(\"Name,Code,ISO\\n\")\n",
    "\n",
    "# Scrape data and write to the CSV file\n",
    "for row in rows:\n",
    "    country_name = row.find('a').text\n",
    "    country_code = row.find_all('td')[1].text\n",
    "    iso_codes = row.find_all('td')[2].text\n",
    "    with open(fileName, 'a') as file:\n",
    "        file.write(country_name.lower() + \",\" + parseCountryCode(country_code).lower() + \",\" + parseIsoCodes(iso_codes).lower() + \"\\n\")\n",
    "\n",
    "# Function to replace a specific row in the file\n",
    "def replace_row_in_file(row_number, new_content):\n",
    "    # Read all lines from the file\n",
    "    with open(fileName, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Check if the row_number is within the range of the file\n",
    "    if 0 <= row_number < len(lines):\n",
    "        # Replace the specific row\n",
    "        lines[row_number] = new_content + '\\n'\n",
    "    \n",
    "    # Write the modified lines back to the file\n",
    "    with open(fileName, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "replace_row_in_file(60, \"dominican republic,1809,do\")\n",
    "replace_row_in_file(171, \"puerto rico,1787,pr\")\n",
    "\n",
    "countries=pd.read_csv(\"countryCodes.csv\",on_bad_lines=\"warn\",dtype=str,quotechar='\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data about countries thanks to \"https://countrycode.org/\" . We complete the missing data as explained before. There may be some problems with this aproach since im suppose to merge(maybe the original data had russia and the countryCodes.csv has russian federation , but we'll solve this later) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(\"facebook.csv\", quotechar='\"', on_bad_lines='warn', engine='python',dtype=str)\n",
    "\n",
    "data['country_name'] = data['country_name'].str.lower()\n",
    "data['country_code'] = data['country_code'].str.lower()\n",
    "\n",
    "# Process each record\n",
    "for index, record in data.iterrows():\n",
    "    country_name = record[\"country_name\"].strip() if isinstance(record[\"country_name\"], str) else ''\n",
    "    if country_name != '':\n",
    "        country_code = countries[countries['Name'] == country_name]['ISO']\n",
    "        if not country_code.empty:\n",
    "            data.at[index, \"country_code\"] = country_code.values[0]\n",
    "    country_code = record[\"country_code\"].strip() if isinstance(record[\"country_code\"], str) else ''\n",
    "    if country_code != '':\n",
    "        country_name = countries[countries['ISO'] == country_code]['Name']\n",
    "        if not country_name.empty:\n",
    "            data.at[index, \"country_name\"] = country_name.values[0]\n",
    "\n",
    "    \n",
    "data.to_csv(\"facebook.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to format the phone number .We need to take in conisderation the case when the number is printed as a scientific notation , the case when the number of characters is diffrent than 12 and the case wwhen the phone number doesnt start with the usual characters for the country from which the phone number is posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a total of 1651 wrong numbers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"facebook.csv\", quotechar='\"', on_bad_lines='warn', engine='python',dtype=str)\n",
    "\n",
    "\n",
    "def is_scientific_notation(value):\n",
    "    pattern = r'^\\d+\\.?\\d*E[+-]?\\d+$'\n",
    "    return bool(re.match(pattern, value))\n",
    "\n",
    "def format_phone_number(sci_notation):\n",
    "    phone_number = float(sci_notation)\n",
    "    phone_str = str(int(phone_number))\n",
    "    if len(phone_str) >= 10:\n",
    "        return str(f\"{phone_str[:3]}{phone_str[3:6]}{phone_str[6:]}\")\n",
    "    else:\n",
    "        return \"Invalid phone number length\"\n",
    "\n",
    "\n",
    "nrOfWrongNumbers=0\n",
    "\n",
    "def transformDataFrameIntoArray(dataframe):\n",
    "    value = []\n",
    "    for i in range(len(dataframe)):\n",
    "        value.extend(dataframe.iloc[i].tolist())  # Append values one by one\n",
    "    return value\n",
    "\n",
    "for index,record in data.iterrows():\n",
    "    recordPhone=str(record['phone'])\n",
    "    recordCountry=str(record['country_name'])\n",
    "    phoneCodeCountry=transformDataFrameIntoArray(countries[countries[\"Name\"]==recordCountry])[1] if countries[countries[\"Name\"]==recordCountry].empty==False else \"\"\n",
    "    # print(recordPhone+\"     \"+phoneCodeCountry+\"     \"+recordCountry)\n",
    "    if(is_scientific_notation(recordPhone)):\n",
    "        recordPhone=\"+\"+format_phone_number(recordPhone)\n",
    "        data.at[index,\"phone\"]=recordPhone\n",
    "    if(recordPhone!=\"nan\" and recordPhone!=\"\"):\n",
    "        if(recordPhone[1:].startswith(str(phoneCodeCountry))==False or len(recordPhone)!=12):\n",
    "            nrOfWrongNumbers=nrOfWrongNumbers+1  \n",
    "            data.at[index,\"phone\"]=\"WRONG\"\n",
    "\n",
    "print(f\"we have a total of {nrOfWrongNumbers} wrong numbers\")\n",
    "data.to_csv(\"facebook.csv\", index=False,encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will erase the unnecesary data from the adress column. In many cases the adress column contains data from other columns asweel , like zipcode , country name , country code and so on...\n",
    "        ex:\n",
    "            Before :134 rue entrepreneurs, za du vigné, 30420, calvisson, france, languedoc-roussillon\n",
    "            After  :134 rue entrepreneurs,za du vigné,languedoc-roussillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV in chunks\n",
    "chunk_size = 1000  # Adjust based on your system's capabilities\n",
    "chunks = pd.read_csv(\"facebook.csv\", dtype=str, engine=\"python\", quotechar='\"', on_bad_lines=\"warn\", chunksize=chunk_size)\n",
    "\n",
    "def transformArrayintoString(array):\n",
    "    value=\"\"\n",
    "    for elem in array:\n",
    "        value=value+elem+\",\"\n",
    "    return value[:-1]\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in chunks:\n",
    "    chunk = chunk.fillna('')\n",
    "    for index, record in chunk.iterrows():\n",
    "        adresaDetailArrayInitial = record[\"address\"].split(\",\")\n",
    "        adresaDetailArrayNew = []\n",
    "        for elem in adresaDetailArrayInitial:\n",
    "            elem = elem.strip()\n",
    "            exista = False\n",
    "            for column in chunk.columns:\n",
    "                if column != \"address\":\n",
    "                    if chunk.loc[index, column].strip() == elem:\n",
    "                        exista = True\n",
    "            if not exista:\n",
    "                adresaDetailArrayNew.append(elem)\n",
    "                \n",
    "        chunk.at[index, \"address\"] = transformArrayintoString(adresaDetailArrayNew) if len(adresaDetailArrayNew) != 0 else chunk.at[index, \"address\"]\n",
    "\n",
    "    # Write the processed chunk to a new CSV file without the index\n",
    "    chunk.to_csv(\"facebookCHUNKY.csv\", mode='a', header=not (chunk.index[0] > 0), encoding=\"utf-8-sig\", index=False)\n",
    "chunkyData=pd.read_csv(\"facebookCHUNKY.csv\",on_bad_lines=\"warn\",engine=\"python\",quotechar='\"',dtype=str)\n",
    "chunkyData.to_csv(\"facebook.csv\",encoding=\"utf-8-sig\",quotechar='\"',mode='w',index=False)\n",
    "os.remove(path=\"facebookCHUNKY.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go ahead and check all the emails, using the nastiest regex i ever saw in my life . If the email doesnt have the correct form , we change it to \"WRONG\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 26 incorrectly formated emails\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"facebook.csv\", engine=\"python\", quotechar='\"', on_bad_lines=\"warn\",dtype=str)\n",
    "pattern = r\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\\\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\\\])\"\n",
    "nrOfWrongEmails=0\n",
    "# Check each email\n",
    "for index, record in df.iterrows():\n",
    "    email = str(record[\"email\"])  # Convert to string\n",
    "    if not re.match(pattern, email) and email!=\"nan\" and email!=\"\":\n",
    "        df.at[index,\"email\"]=\"WRONG\"\n",
    "        nrOfWrongEmails=nrOfWrongEmails+1\n",
    "print(f\"We have a total of {nrOfWrongEmails} incorrectly formated emails\")\n",
    "df.to_csv(\"facebook.csv\",encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the links, using a regex. If the link doesnt have the correct form we change it to \"WRONG\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 133 incorrectly formated links\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"facebook.csv\", engine=\"python\", quotechar='\"', on_bad_lines=\"warn\",dtype=str)\n",
    "pattern=\"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "nrOfWrongLinks=0\n",
    "for index,record in df.iterrows():\n",
    "    link=record[\"link\"]\n",
    "    if not re.match(pattern,link) and link!=\"nan\" and link!=\"\":\n",
    "        df.at[index,\"link\"]=\"WRONG\"\n",
    "        nrOfWrongLinks=nrOfWrongLinks+1\n",
    "print(f\"We have a total of {nrOfWrongLinks} incorrectly formated links\")\n",
    "df.to_csv(\"facebook.csv\",encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a smart approach, we should've drop duplicates from the begging , not at the end, but we learn . (Luckily there are no duplicates , so it doesnt affect us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"facebook.csv\", dtype=str, engine=\"python\", quotechar='\"', on_bad_lines=\"warn\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "# `subset=None` checks all columns; you can specify column names if needed\n",
    "duplicates = df[df.duplicated(subset=None)]\n",
    "\n",
    "# Print the duplicates\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicate rows:\\n\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried to fill in the values of zipcodes for the records of which we know the city , but it is a mess, the alrgorithm takes way to long, and is unale to finish ,and some result differ than the ones from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from geopy.geocoders import Nominatim\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"facebook_cleaner\")\n",
    "\n",
    "    # location=geolocator.geocode(\"calvisson\",addressdetails=True)\n",
    "    # print(location.raw[\"address\"][\"postcode\"])\n",
    "    df=pd.read_csv(\"facebook.csv\",dtype=str,quotechar='\"',engine=\"python\",on_bad_lines=\"warn\")\n",
    "    df.fillna('',inplace=True)\n",
    "    uniqueCities=df[\"city\"].unique()\n",
    "    with open(\"codesVsActualCode.txt\", 'w') as file:\n",
    "        for city in uniqueCities:\n",
    "            try:\n",
    "                cityZipCodeGEO=geolocator.geocode(city,addressdetails=True).raw[\"address\"][\"postcode\"]  \n",
    "                file.writelines(f\"The name of the city is {city};using geopy we get {cityZipCodeGEO}\\n\")\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "            except AttributeError as e2:\n",
    "                print(e2)\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However , seeing the shape of the address column there must be a way to actually do this ( fill in region , region code , zip code ...) . We start with what seems to look like a city name\n",
    "example:2989 unity road,k0h 1m0,kingston,on,ontario\n",
    "2989 unity road -address\n",
    "k0h 1m0 -zipcode\n",
    "on - region code\n",
    "ontario -region name\n",
    "kingston -city\n",
    "WE WILL FIND A WAY !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did indeed found a way . however , if a certain zipcode is both zipcode and region name we may have some problems . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry friend , but it seems that 134 Rue Entrepreneurs is not a city for fr\n",
      "Sorry friend , but it seems that 601 Cordova St W # 270 is not a city for ca\n",
      "Sorry friend , but it seems that Unit 3 - 4 Donald Street is not a city for ca\n",
      "Sorry friend , but it seems that 307 Main Street is not a city for ca\n",
      "Sorry friend , but it seems that 8623 Granville Street Unit 143 is not a city for ca\n",
      "Sorry friend , but it seems that 2989 Unity Road is not a city for ca\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 201\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m(is_city_in_country(elem,countryCodeRecord)):\n\u001b[0;32m    200\u001b[0m                 df\u001b[38;5;241m.\u001b[39mat[index,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39melem\n\u001b[1;32m--> 201\u001b[0m                 \u001b[43madressDetails\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[index,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mtransformArrayintoString(adressDetails)\n\u001b[0;32m    204\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebookFINAL.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "\n",
    "import pycountry\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# def is_city_in_country(city_name, country_iso2):\n",
    "#     geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "#     location = geolocator.geocode(city_name + \", \" + country_iso2)\n",
    "\n",
    "#     if location:                                                                                  #geopi = not so great . limited requests , i need to find an alternative\n",
    "#         # Check if the location's country matches the ISO2 code\n",
    "#         if location.raw['address'].get('country_code', '').upper() == country_iso2.upper():\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def is_city_in_country(cityName,countryCode):\n",
    "#     cityName=cityName.title()\n",
    "#     response = requests.request(\"GET\", f\"https://www.geonames.org/search.html?q={cityName}&country={countryCode}\")\n",
    "#     try:                                                                                                                  #ontario is both city and region , so we have a problem\n",
    "#         country = re.findall(\"/countries.*\\\\.html\", response.text)[0].strip(\".html\").split(\"/\")[-1]\n",
    "#     except IndexError as e:\n",
    "#         print(f\"Sorry friend , but it seems that {cityName} is not a city for {countryCode}\")\n",
    "#     return True\n",
    "\n",
    "\n",
    "def is_region_name(regionName, countryCode):\n",
    "    if(pycountry.subdivisions.get(country_code=countryCode)is not None):\n",
    "        for region in pycountry.subdivisions.get(country_code=countryCode):\n",
    "            if regionName.lower() == region.name.lower():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def is_zip_code(zipCode, countryCode):\n",
    "    zipCode = zipCode.upper()\n",
    "    countryCode = countryCode.upper()\n",
    "    realZipCodePattern = zipPatterns[zipPatterns['Code'] == countryCode]['Pattern']    \n",
    "    if not realZipCodePattern.empty:\n",
    "        pattern = realZipCodePattern.iloc[0]  # Get the first pattern as a string\n",
    "        # print(f\"Pattern for {countryCode}: {pattern}\")\n",
    "        return re.match(pattern, zipCode) is not None  # Return True or False\n",
    "    return False  # Return False if no pattern is found\n",
    "\n",
    "def is_country_code(countryCode):\n",
    "    countryCode=countryCode.upper()\n",
    "    if not zipPatterns[zipPatterns['Code'] == countryCode].empty:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_country_name(countryName):\n",
    "    countryName=countryName.lower()\n",
    "    return countryName in countries[\"Name\"].values\n",
    "\n",
    "def is_region_code(region_code):\n",
    "    region_code = region_code.upper()\n",
    "    for subdivision in pycountry.subdivisions:\n",
    "        if subdivision.code == region_code:\n",
    "            return True  \n",
    "    return False \n",
    "\n",
    "def country_in_array(array):\n",
    "    for elem in array:\n",
    "        if(is_country_name(elem)==True):\n",
    "            return elem\n",
    "    return False\n",
    " \n",
    "def countryCode_in_array(array):\n",
    "    for elem in array:\n",
    "        if(is_country_code(elem)==True):\n",
    "            return elem\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def get_region_code(region_name, country_code):\n",
    "    region_name = region_name.lower()\n",
    "    country_code = country_code.upper()\n",
    "    if(pycountry.subdivisions.get(country_code=country_code)is not None):\n",
    "        for subdivision in pycountry.subdivisions.get(country_code=country_code):\n",
    "            # Check if the subdivision's name matches the region name\n",
    "            if subdivision.name.lower() == region_name:\n",
    "                return subdivision.code.split('-')[1]  # Return the region code\n",
    "    return None\n",
    "\n",
    "\n",
    "zipPatterns = pd.read_csv(\"regexZipCodesForCountries.csv\", engine=\"python\", dtype=str, on_bad_lines=\"warn\", quotechar='\"', escapechar='௸')\n",
    "countries=pd.read_csv(\"countryCodes.csv\", engine=\"python\", dtype=str, on_bad_lines=\"warn\", quotechar='\"')\n",
    "df=pd.read_csv(\"facebook.csv\",dtype=str,engine=\"python\",on_bad_lines=\"warn\",quotechar='\"')\n",
    "df.fillna('',inplace=True)\n",
    "for index,record in df.iterrows():\n",
    "    adressDetails=record[\"address\"].split(',')\n",
    "    if(record[\"country_name\"]==\"\"):\n",
    "        countryName=country_in_array(adressDetails)\n",
    "        if(countryName!=False):\n",
    "            df.at[index,\"country_name\"]=countryName\n",
    "            df.at[index,\"country_code\"]=countries[countries[\"Name\"]==countryName][\"ISO\"].values[0]\n",
    "            adressDetails.remove(countryName)\n",
    "            if(countries[countries[\"Name\"]==countryName][\"ISO\"].values[0] in adressDetails):\n",
    "                adressDetails.remove(countries[countries[\"Name\"]==countryName][\"ISO\"].values[0])\n",
    "        else:\n",
    "            countryCode=countryCode_in_array(adressDetails)\n",
    "            if(countryCode!=False):\n",
    "                df.at[index,\"country_code\"]=countryCode\n",
    "                df.at[index,\"country_name\"]=countries[countries[\"ISO\"]==countryCode][\"Name\"].values[0]\n",
    "                adressDetails.remove(countryCode)\n",
    "                if(countries[countries[\"ISO\"]==countryCode][\"Name\"].values[0] in adressDetails):\n",
    "                    adressDetails.remove(countries[countries[\"ISO\"]==countryCode][\"Name\"].values[0])\n",
    "    record[\"address\"]=transformArrayintoString(adressDetails)\n",
    "\n",
    "for index,record in df.iterrows():\n",
    "    adressDetails=record[\"address\"].split(',')\n",
    "    for elem in adressDetails:\n",
    "        if(record[\"country_code\"]!=\"\"):\n",
    "            countryCodeRecord=record[\"country_code\"]\n",
    "            if(is_region_name(elem,countryCodeRecord) and (record[\"region_name\"]!=\"\" or is_region_name(record[\"region_name\"],countryCodeRecord)==False)):                                      #region name\n",
    "                df.at[index,\"region_name\"]=elem\n",
    "                df.at[index,\"region_code\"]=get_region_code(elem,countryCodeRecord).lower()\n",
    "                adressDetails.remove(elem)\n",
    "                if(get_region_code(elem,countryCodeRecord).lower() in adressDetails):\n",
    "                    adressDetails.remove(get_region_code(elem,countryCodeRecord).lower())\n",
    "            if(is_zip_code(elem,countryCodeRecord) and (record[\"zip_code\"]!=\"\" or is_zip_code(record[\"zip_code\"],countryCodeRecord)==False)):                           \n",
    "                df.at[index,\"zip_code\"]=elem\n",
    "                adressDetails.remove(elem)\n",
    "\n",
    "    df.at[index,\"address\"]=transformArrayintoString(adressDetails)\n",
    "\n",
    "df.to_csv(\"facebookFINAL.csv\",encoding=\"utf-8-sig\",index=False)\n",
    "            \n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
