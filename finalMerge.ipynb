{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['language', 'phone', 'site_name', 'address', 'country_code',\n",
      "       'raw_address', 'raw_phone', 'region_code', 'text', 'zip_code',\n",
      "       'number_reviews', 'customer_rating', 'years_experience', 'domain',\n",
      "       'name', 'city', 'country_name', 'region_name'],\n",
      "      dtype='object')\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "facebookData = pd.read_csv(\"facebookFINAL.csv\", quotechar='\"', dtype=str, on_bad_lines=\"warn\", engine=\"python\")\n",
    "googleData = pd.read_csv(\"google.csv\", quotechar='\"', dtype=str, on_bad_lines=\"warn\", engine=\"python\")\n",
    "websiteData = pd.read_csv(\"website.csv\", quotechar='\"', dtype=str, on_bad_lines=\"warn\", engine=\"python\")\n",
    "\n",
    "def resolve_conflicts(row, col_x, col_y):\n",
    "    if pd.isna(row[col_x]) or row[col_x] == '':\n",
    "        return row[col_y]\n",
    "    return row[col_x]\n",
    "\n",
    "# Adjust chunk size based on memory\n",
    "chunk_size = 100000  # Adjust based on memory limits\n",
    "\n",
    "# Initialize an empty DataFrame to hold merged results\n",
    "merged_results = pd.DataFrame()\n",
    "\n",
    "# Process in chunks for websiteData\n",
    "for chunk in pd.read_csv(\"website.csv\", chunksize=chunk_size, quotechar='\"', dtype=str, on_bad_lines=\"warn\"):\n",
    "    # Rename columns in websiteData to match googleData for merging\n",
    "    chunk.rename(columns={\"root_domain\": \"domain\", \n",
    "                          \"legal_name\": \"name\", \n",
    "                          \"main_city\": \"city\", \n",
    "                          \"main_country\": \"country_name\", \n",
    "                          \"main_region\": \"region_name\", \n",
    "                          \"s_category\": \"category\"}, inplace=True)\n",
    "\n",
    "    # Fill missing values with an empty string to avoid NaN issues during merge\n",
    "    chunk.fillna('', inplace=True)\n",
    "\n",
    "    # Ensure certain columns are lowercase for consistency\n",
    "    chunk[\"name\"] = chunk[\"name\"].apply(str.lower)\n",
    "    chunk[\"region_name\"] = chunk[\"region_name\"].apply(str.lower)\n",
    "    chunk[\"country_name\"] = chunk[\"country_name\"].apply(str.lower)\n",
    "    chunk[\"category\"] = chunk[\"category\"].apply(str.lower)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    chunk.drop(columns=[\"domain_suffix\", \"tld\"], inplace=True)\n",
    "\n",
    "    # Merge websiteData chunk with googleData based on common columns\n",
    "    merged_chunk = pd.merge(chunk, googleData, how='right', on=[\"phone\"], suffixes=('_x', '_y'))\n",
    "\n",
    "    # Apply conflict resolution for necessary columns after merging\n",
    "    if not merged_results.empty:\n",
    "        merged_chunk['domain'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'domain_x', 'domain_y'), axis=1)\n",
    "        merged_chunk['name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'name_x', 'name_y'), axis=1)\n",
    "        merged_chunk['city'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'city_x', 'city_y'), axis=1)\n",
    "        merged_chunk['country_name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'country_name_x', 'country_name_y'), axis=1)\n",
    "        merged_chunk['region_name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'region_name_x', 'region_name_y'), axis=1)\n",
    "    else:\n",
    "        # If merged_results is empty, create the new columns directly\n",
    "        merged_chunk['domain'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'domain_x', 'domain_y'), axis=1)\n",
    "        merged_chunk['name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'name_x', 'name_y'), axis=1)\n",
    "        merged_chunk['city'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'city_x', 'city_y'), axis=1)\n",
    "        merged_chunk['country_name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'country_name_x', 'country_name_y'), axis=1)\n",
    "        merged_chunk['region_name'] = merged_chunk.apply(lambda row: resolve_conflicts(row, 'region_name_x', 'region_name_y'), axis=1)\n",
    "\n",
    "    # Concatenate the merged chunk with the merged_results DataFrame\n",
    "    merged_results = pd.concat([merged_results, merged_chunk], ignore_index=True)\n",
    "\n",
    "# Drop all columns ending with _x and _y\n",
    "merged_results = merged_results.loc[:, ~merged_results.columns.str.endswith(('_x', '_y'))]\n",
    "merged_results=merged_results.drop(columns=[\"phone_country_code\"])\n",
    "\n",
    "\n",
    "# Save the final merged results to a CSV file\n",
    "merged_results.to_csv(\"anomalie.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# Print final column names and length\n",
    "print(merged_results.columns)\n",
    "print(len(merged_results.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 506. MiB for an array with shape (19, 3491664) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m facebook_chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebookFINAL.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39mchunk_size, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Process merged_results in chunks\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m merged_chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manomalie.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39mchunk_size, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m         \n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# Merge the current chunk of merged_results with the current chunk of facebookData\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m         chunk_merged \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacebook_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_fb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Resolve conflicts for necessary columns\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         chunk_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk_merged\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: resolve_conflicts(row, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_fb\u001b[39m\u001b[38;5;124m'\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    877\u001b[0m left\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m llabels\n\u001b[0;32m    878\u001b[0m right\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m rlabels\n\u001b[1;32m--> 879\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\Desktop\\Portofoliu\\DataEngineerTask\\venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 506. MiB for an array with shape (19, 3491664) and data type object"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Set chunk size\n",
    "# chunk_size = 6000  # Adjust based on your memory constraints\n",
    "\n",
    "# # Initialize an empty DataFrame to hold the final results\n",
    "# final_results = pd.DataFrame()\n",
    "\n",
    "# # Process facebookData in chunks\n",
    "# for facebook_chunk in pd.read_csv(\"facebookFINAL.csv\", chunksize=chunk_size, quotechar='\"', dtype=str, on_bad_lines=\"warn\"):\n",
    "    \n",
    "#     # Process merged_results in chunks\n",
    "#     for merged_chunk in pd.read_csv(\"anomalie.csv\", chunksize=chunk_size, quotechar='\"', dtype=str, on_bad_lines=\"warn\"):\n",
    "        \n",
    "#         # Merge the current chunk of merged_results with the current chunk of facebookData\n",
    "#         chunk_merged = pd.merge(merged_chunk, facebook_chunk, how='left', on=[\"phone\"], suffixes=('', '_fb'))\n",
    "        \n",
    "#         # Resolve conflicts for necessary columns\n",
    "#         chunk_merged['domain'] = chunk_merged.apply(lambda row: resolve_conflicts(row, 'domain', 'domain_fb'), axis=1)\n",
    "#         chunk_merged['name'] = chunk_merged.apply(lambda row: resolve_conflicts(row, 'name', 'name_fb'), axis=1)\n",
    "#         chunk_merged['city'] = chunk_merged.apply(lambda row: resolve_conflicts(row, 'city', 'city_fb'), axis=1)\n",
    "#         chunk_merged['country_name'] = chunk_merged.apply(lambda row: resolve_conflicts(row, 'country_name', 'country_name_fb'), axis=1)\n",
    "#         chunk_merged['region_name'] = chunk_merged.apply(lambda row: resolve_conflicts(row, 'region_name', 'region_name_fb'), axis=1)\n",
    "\n",
    "#         # Drop the columns ending with _fb\n",
    "#         chunk_merged = chunk_merged.loc[:, ~chunk_merged.columns.str.endswith('_fb')]\n",
    "\n",
    "#         # Concatenate the chunk to final_results\n",
    "#         final_results = pd.concat([final_results, chunk_merged], ignore_index=True)\n",
    "\n",
    "# # Save the final results to a new CSV file\n",
    "# final_results.to_csv(\"final_anomalie.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# # Print final column names and length\n",
    "# print(final_results.columns)\n",
    "# print(len(final_results.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
